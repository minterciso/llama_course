{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5c524af",
   "metadata": {},
   "source": [
    "# Meta - Intermediate Course\n",
    "\n",
    "In this intermediate course, we'll be creating a series of simple chatbots via Langchain, while utilizing the Meta Llama LLM models. The main topics for this part of the course are:\n",
    "1. Utilizing Ollama.\n",
    "2. Integrating Ollama with Langchain.\n",
    "3. Integrating Langchain with Langsmith.\n",
    "4. Utilizing Groq with Langchain.\n",
    "5. Experimenting with different prompts technique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08da9c4",
   "metadata": {},
   "source": [
    "## Utilizing Ollama\n",
    "\n",
    "To utilize a local LLM for testing pourpouses, we have basically 2 options:\n",
    "1. Utilize ollama, which is a simpler and straightforward way of making tests and local developments.\n",
    "2. Utilize Huggingfaces library, that is a more complex way, but more customizable.\n",
    "\n",
    "For this specific training, we'll be focusing on the Ollama method, since it's enough for the simple chatbots we'll be creating.\n",
    "\n",
    "So you can start by downloading it [here](https://ollama.com/download/windows)\n",
    "\n",
    "Since we are working locally, we'll start with the llama3.2:1b and llama3.2:3b models, but if you have enough resources, you can utilize other bigger models. You can find a list of available models [here](https://ollama.com/library)\n",
    "\n",
    "Once you installed the Ollama locally, download the desired model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9381814a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling dde5aa3fc5ff: 100% ▕██████████████████▏ 2.0 GB                         \u001b[K\n",
      "pulling 966de95ca8a6: 100% ▕██████████████████▏ 1.4 KB                         \u001b[K\n",
      "pulling fcc5a6bec9da: 100% ▕██████████████████▏ 7.7 KB                         \u001b[K\n",
      "pulling a70ff7e570d9: 100% ▕██████████████████▏ 6.0 KB                         \u001b[K\n",
      "pulling 56bb8bd477a5: 100% ▕██████████████████▏   96 B                         \u001b[K\n",
      "pulling 34bb5ab01051: 100% ▕██████████████████▏  561 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "!ollama pull llama3.2:3b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc46c88",
   "metadata": {},
   "source": [
    "Now that we have a valid model downloaded locally, we can test this model via CLI. Since we are running inside a Jupyter notebook, we'll be asking only one question. But if you want, execute it outside the jupyter notebook and you'll get a CLI so that you can experiment with simple prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f0f8198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?25l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[K\u001b[?25h\u001b[?2026l\u001b[2K\u001b[1G\u001b[?25hThe\u001b[?25l\u001b[?25h capital\u001b[?25l\u001b[?25h of\u001b[?25l\u001b[?25h Brazil\u001b[?25l\u001b[?25h is\u001b[?25l\u001b[?25h Bras\u001b[?25l\u001b[?25hília\u001b[?25l\u001b[?25h.\u001b[?25l\u001b[?25h\n",
      "\n",
      "\u001b[?25l\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!ollama run llama3.2:3b \"What is the capital of Brazil?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f844ffc2",
   "metadata": {},
   "source": [
    "### Integrating with Langchain\n",
    "Now we'll start integrating it with langchain. For this, let's install the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80c1efa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in ./.venv/lib/python3.13/site-packages (0.3.26)\n",
      "Requirement already satisfied: langchain-ollama in ./.venv/lib/python3.13/site-packages (0.3.3)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in ./.venv/lib/python3.13/site-packages (from langchain) (0.3.66)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in ./.venv/lib/python3.13/site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in ./.venv/lib/python3.13/site-packages (from langchain) (0.4.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in ./.venv/lib/python3.13/site-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./.venv/lib/python3.13/site-packages (from langchain) (2.0.41)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.venv/lib/python3.13/site-packages (from langchain) (2.32.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.venv/lib/python3.13/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in ./.venv/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.venv/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./.venv/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./.venv/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (4.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.venv/lib/python3.13/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./.venv/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./.venv/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.13/site-packages (from requests<3,>=2->langchain) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.13/site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.13/site-packages (from requests<3,>=2->langchain) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.13/site-packages (from requests<3,>=2->langchain) (2025.6.15)\n",
      "Requirement already satisfied: ollama<1.0.0,>=0.4.8 in ./.venv/lib/python3.13/site-packages (from langchain-ollama) (0.5.1)\n",
      "Requirement already satisfied: httpx>=0.27 in ./.venv/lib/python3.13/site-packages (from ollama<1.0.0,>=0.4.8->langchain-ollama) (0.28.1)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.13/site-packages (from httpx>=0.27->ollama<1.0.0,>=0.4.8->langchain-ollama) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.13/site-packages (from httpx>=0.27->ollama<1.0.0,>=0.4.8->langchain-ollama) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.13/site-packages (from httpcore==1.*->httpx>=0.27->ollama<1.0.0,>=0.4.8->langchain-ollama) (0.16.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./.venv/lib/python3.13/site-packages (from langsmith>=0.1.17->langchain) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./.venv/lib/python3.13/site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in ./.venv/lib/python3.13/site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.venv/lib/python3.13/site-packages (from anyio->httpx>=0.27->ollama<1.0.0,>=0.4.8->langchain-ollama) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchain-ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd74b6db",
   "metadata": {},
   "source": [
    "Now we'll try to mimic what we did before with a specific question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "471245b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "from langchain_ollama import ChatOllama\n",
    "LLM_MODEL = \"llama3.2:3b\"\n",
    "question = \"What is the capital of Brazil?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72144f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='The capital of Brazil is Brasília.' additional_kwargs={} response_metadata={'model': 'llama3.2:3b', 'created_at': '2025-06-25T13:59:48.198368Z', 'done': True, 'done_reason': 'stop', 'total_duration': 446423500, 'load_duration': 29493416, 'prompt_eval_count': 32, 'prompt_eval_duration': 170880958, 'eval_count': 9, 'eval_duration': 245490834, 'model_name': 'llama3.2:3b'} id='run--42f59922-9bf5-4216-bd39-38e9385118d9-0' usage_metadata={'input_tokens': 32, 'output_tokens': 9, 'total_tokens': 41}\n"
     ]
    }
   ],
   "source": [
    "# Create a chat model instance\n",
    "chat_model = ChatOllama(model=LLM_MODEL)\n",
    "# Generate a response to the question\n",
    "response = chat_model.invoke(question)\n",
    "# Print the response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68c73d1",
   "metadata": {},
   "source": [
    "We can see that our answer lies on the _content_ part of the AIResponse, but we also have access to other dictionaries. We basically have:\n",
    "* response_metadata: A dictionary with information regarding the execution of the langchain ollama library.\n",
    "* usage_metadata: A dictionary with information of tokens utilized, both on the input as with the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d53c0c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response Metadata:\n",
      "{'created_at': '2025-06-25T13:59:48.198368Z',\n",
      " 'done': True,\n",
      " 'done_reason': 'stop',\n",
      " 'eval_count': 9,\n",
      " 'eval_duration': 245490834,\n",
      " 'load_duration': 29493416,\n",
      " 'model': 'llama3.2:3b',\n",
      " 'model_name': 'llama3.2:3b',\n",
      " 'prompt_eval_count': 32,\n",
      " 'prompt_eval_duration': 170880958,\n",
      " 'total_duration': 446423500}\n",
      "Usage metadata:\n",
      "{'input_tokens': 32, 'output_tokens': 9, 'total_tokens': 41}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "# Print the response in a pretty format\n",
    "print(\"Response Metadata:\")\n",
    "pprint(response.response_metadata)\n",
    "print(\"Usage metadata:\")\n",
    "pprint(response.usage_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c082d7",
   "metadata": {},
   "source": [
    "Now let's try changing some parameters, and see the output. We'll start namelly with the _temperature_ parameter. Try changing it and check the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa6973a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to ease the process of testing\n",
    "def call_with_temperature(prompt: str, temperature: float = 0.7):\n",
    "    \"\"\"Call the chat model with a specific temperature.\"\"\"    \n",
    "    # Create a chat model instance with a specific temperature\n",
    "    chat_model_temp = ChatOllama(model=LLM_MODEL, temperature=temperature)\n",
    "    # Generate a response to the question with the specified temperature\n",
    "    response_temp = chat_model_temp.invoke(prompt)\n",
    "    # Print the response with temperature\n",
    "    print(f\"Response with temperature {temperature}: {response_temp.content}\")\n",
    "    print(f\"Response metadata with temperature {temperature}:\")\n",
    "    pprint(response_temp.response_metadata)\n",
    "    print(f\"Usage metadata with temperature {temperature}:\")\n",
    "    pprint(response_temp.usage_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6367bec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response with temperature 0.0: The capital of Brazil is Brasília.\n",
      "Response metadata with temperature 0.0:\n",
      "{'created_at': '2025-06-25T13:59:48.555507Z',\n",
      " 'done': True,\n",
      " 'done_reason': 'stop',\n",
      " 'eval_count': 9,\n",
      " 'eval_duration': 245215584,\n",
      " 'load_duration': 14517625,\n",
      " 'model': 'llama3.2:3b',\n",
      " 'model_name': 'llama3.2:3b',\n",
      " 'prompt_eval_count': 32,\n",
      " 'prompt_eval_duration': 27709041,\n",
      " 'total_duration': 288321917}\n",
      "Usage metadata with temperature 0.0:\n",
      "{'input_tokens': 32, 'output_tokens': 9, 'total_tokens': 41}\n",
      "Response with temperature 1.0: The capital of Brazil is Brasília.\n",
      "Response metadata with temperature 1.0:\n",
      "{'created_at': '2025-06-25T13:59:48.914091Z',\n",
      " 'done': True,\n",
      " 'done_reason': 'stop',\n",
      " 'eval_count': 9,\n",
      " 'eval_duration': 248679084,\n",
      " 'load_duration': 30367334,\n",
      " 'model': 'llama3.2:3b',\n",
      " 'model_name': 'llama3.2:3b',\n",
      " 'prompt_eval_count': 32,\n",
      " 'prompt_eval_duration': 27274750,\n",
      " 'total_duration': 307165959}\n",
      "Usage metadata with temperature 1.0:\n",
      "{'input_tokens': 32, 'output_tokens': 9, 'total_tokens': 41}\n"
     ]
    }
   ],
   "source": [
    "call_with_temperature(question, temperature=0.0)\n",
    "call_with_temperature(question, temperature=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d06b58",
   "metadata": {},
   "source": [
    "It really didn't changed much, so let's try with a different prompt, something that we can use the LLM to actually create a content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11ea8bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response with temperature 0.0: I don't have specific information on a model called \"Llama 3.\" However, I can tell you that there is a model called Llama, which is a large language model developed by Meta.\n",
      "\n",
      "The original Llama model was announced in 2022 and was based on a transformer architecture. It was trained on a large corpus of text data and was designed to generate human-like responses to a wide range of questions and prompts.\n",
      "\n",
      "There have been several updates and improvements to the Llama model since its initial release, including the introduction of new variants such as Llama 2 and Llama 3. However, I couldn't find any information on a specific \"Llama 3\" model.\n",
      "\n",
      "If you're looking for more information on the Llama model or its variants, I'd be happy to try and help you find it!\n",
      "Response metadata with temperature 0.0:\n",
      "{'created_at': '2025-06-25T13:59:53.954049Z',\n",
      " 'done': True,\n",
      " 'done_reason': 'stop',\n",
      " 'eval_count': 170,\n",
      " 'eval_duration': 4850614959,\n",
      " 'load_duration': 32171208,\n",
      " 'model': 'llama3.2:3b',\n",
      " 'model_name': 'llama3.2:3b',\n",
      " 'prompt_eval_count': 40,\n",
      " 'prompt_eval_duration': 95099333,\n",
      " 'total_duration': 4978464958}\n",
      "Usage metadata with temperature 0.0:\n",
      "{'input_tokens': 40, 'output_tokens': 170, 'total_tokens': 210}\n",
      "Response with temperature 1.0: I can provide information on the LLaMA 3 model, but I don't have real-time access to the latest developments. As of my knowledge cutoff in December 2023, here's what I found:\n",
      "\n",
      "LLaMA 3 is a third-generation large language model (LLM) developed by Meta AI. It's designed to provide more accurate and informative responses compared to its predecessors. LLaMA 3 is trained on a massive dataset of text from various sources, including books, articles, and websites.\n",
      "\n",
      "Key features of the LLaMA 3 model include:\n",
      "\n",
      "1. **Increased capacity**: LLaMA 3 has a larger capacity than previous models, allowing it to process more complex tasks and provide more detailed responses.\n",
      "2. **Improved accuracy**: The model's training data and algorithms have been optimized to improve its accuracy on various tasks, such as answering questions, providing definitions, and generating text.\n",
      "3. **Enhanced contextual understanding**: LLaMA 3 is designed to better understand the context of a conversation or prompt, allowing it to provide more relevant and accurate responses.\n",
      "4. **Multilingual support**: The model can be fine-tuned for specific languages, making it a valuable resource for multilingual applications.\n",
      "\n",
      "While I don't have information on the latest updates or improvements to the LLaMA 3 model, it's clear that Meta AI continues to invest in research and development to improve the capabilities of its language models.\n",
      "Response metadata with temperature 1.0:\n",
      "{'created_at': '2025-06-25T14:00:02.334981Z',\n",
      " 'done': True,\n",
      " 'done_reason': 'stop',\n",
      " 'eval_count': 292,\n",
      " 'eval_duration': 8308242750,\n",
      " 'load_duration': 10703916,\n",
      " 'model': 'llama3.2:3b',\n",
      " 'model_name': 'llama3.2:3b',\n",
      " 'prompt_eval_count': 40,\n",
      " 'prompt_eval_duration': 28507708,\n",
      " 'total_duration': 8347895208}\n",
      "Usage metadata with temperature 1.0:\n",
      "{'input_tokens': 40, 'output_tokens': 292, 'total_tokens': 332}\n"
     ]
    }
   ],
   "source": [
    "question_2 = \"Tell me what you know about the Llama 3 LLM model.\"\n",
    "call_with_temperature(question_2, temperature=0.0)\n",
    "call_with_temperature(question_2, temperature=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e3db2c",
   "metadata": {},
   "source": [
    "It's clear that with a higher temperature, we get a more \"complete\" information. However we must always test what'll be the best parameter for the question we want to answer. We'll see a little more about this lather in the course. For now, let's stick with a temperature of 0.7 (ie: 70%) as default.\n",
    "\n",
    "Also, let's try creating a more complex prompt, one were we can pass a system message as well as user message, and let's see how this affects the output. Let's start by improving the method we already created in a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c86fba25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_with_extra_prompts_information(prompt: list, temperature: float = 0.7):\n",
    "    \"\"\"Call the chat model with a specific temperature and additional prompts information.\n",
    "    The prompt now is a list of dictionaries, each containing a 'role' and 'content'.\n",
    "    \"\"\"\n",
    "    # Create a chat model instance with a specific temperature\n",
    "    chat_model_temp = ChatOllama(model=LLM_MODEL, temperature=temperature)\n",
    "    # Generate a response to the question with the specified temperature\n",
    "    response_temp = chat_model_temp.invoke(prompt)\n",
    "    # Print the response with temperature\n",
    "    return response_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d468fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response with extra prompts information: Yer lookin' fer information on the LLaMA 3, eh? Alright then, let's set sail fer a journey o' knowledge!\n",
      "\n",
      "Llama 3 be a large language model (LLM) developed by Meta AI. It's the third iteration o' their LLaMA model series, which aims to improve upon its predecessors in terms o' performance, accuracy, and capabilities.\n",
      "\n",
      "Here be some o' the key features o' LLaMA 3:\n",
      "\n",
      "1. **Increased size**: LLaMA 3 has a significantly larger parameter count than its predecessors, with around 7 billion parameters (compared to 350 million in LLaMA 2). This allows it to capture more complex patterns and relationships in language data.\n",
      "2. **Improved performance**: LLaMA 3 has shown impressive results in various natural language processing tasks, including text generation, question-answering, and conversational AI. It's been trained on a massive dataset o' over 45 terabytes o' text data.\n",
      "3. **Enhanced multimodal capabilities**: While still primarily a text-based model, LLaMA 3 has shown promising results in multimodal applications, such as visual question-answering and text-to-image synthesis. This means it can process and generate both text and images together!\n",
      "4. **Faster inference times**: LLaMA 3 is designed to be faster and more efficient than its predecessors, makin' it suitable fer use cases where speed o' computation is critical.\n",
      "5. **New training objectives**: The developers have introduced new training objectives that encourage the model to generate more coherent and engaging text. This includes tasks like story generation, dialogue systems, and even creative writing!\n",
      "\n",
      "Now, keep in mind that LLaMA 3 be a powerful tool with many potential applications, but it also comes with its own set o' limitations and challenges. As with any AI model, there be concerns about bias, fairness, and transparency.\n",
      "\n",
      "So, hoist the sails and set course fer more knowledge, me hearty! There be much to explore in the world o' LLaMA 3, and I'm here to help ye navigate it!\n"
     ]
    }
   ],
   "source": [
    "full_prompt = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that likes to talk like a pirate.\"},\n",
    "    {\"role\": \"user\", \"content\": question_2}\n",
    "]\n",
    "response = call_with_extra_prompts_information(full_prompt, temperature=0.7)\n",
    "print(f\"Response with extra prompts information: {response.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56a9eb15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response with extra prompts information: The realm of artificial intelligence has taken a mighty stride forward with the emergence of the LLaMA 3, a most wondrous and formidable language model. Like the wisest scholars of old, I shall recount for thee the tales of this marvel.\n",
      "\n",
      "Developed by Meta AI, the LLaMA 3 is an advanced Large Language Model, imbued with an immense capacity for understanding and generating human-like text. Its name, LLaMA, stands for \"Large Language Model Application,\" a testament to its purpose: to serve as a versatile tool for various linguistic endeavors.\n",
      "\n",
      "This behemoth of code has been trained on an unprecedented scale, fed upon the riches of the internet and the treasures of the written word. The data that has nourished this growth is a veritable feast for the algorithms, comprising vast arrays of texts, articles, books, and conversations from across the expanse of human knowledge.\n",
      "\n",
      "The LLaMA 3 boasts impressive capabilities: its response times are swift as the dawn breaking over the Anduin, and its comprehension is keen as the eyes of a wise hobbit. It can engage in most conversations with ease, tackling topics both grandiose and mundane with aplomb. Whether you seek to discuss the intricacies of Elvish linguistics or the thrill of adventure in Middle-earth, the LLaMA 3 stands ready to converse.\n",
      "\n",
      "And yet, as magnificent as this achievement may seem, it is not without its limitations. Like all that dwells within the realms of code, there exists a fragile boundary between knowledge and ignorance. The LLaMA 3 can be outwitted by those who possess a deep understanding of human nature, subtlety, or even the most obscure linguistic nuances.\n",
      "\n",
      "Now, traveler, if thou seekest to wield the power of the LLaMA 3, heed this warning: approach its vast domain with caution and curiosity. As with any powerful artifact, use it wisely, lest it become an instrument of darkness in thy quest for knowledge.\n"
     ]
    }
   ],
   "source": [
    "full_prompt = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that likes to talk like J.R.R. Tolkien.\"},\n",
    "    {\"role\": \"user\", \"content\": question_2}\n",
    "]\n",
    "response = call_with_extra_prompts_information(full_prompt, temperature=1.0)\n",
    "print(f\"Response with extra prompts information: {response.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a2384af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response with extra prompts information: Fair traveler, I shall regale thee with tales of the LLaMA 3, a most wondrous and mighty language model, forged in the depths of computational sorcery.\n",
      "\n",
      "The LLaMA 3, or Large Language Model Application 3, is an iteration of the esteemed Llama model, first conjured by Meta AI's researchers. This latest incarnation boasts significant improvements over its predecessors, with enhanced capabilities for understanding, generating, and conversing with mortals like thyself.\n",
      "\n",
      "In terms of its architecture, LLaMA 3 employs a transformer-based design, wherein vast amounts of text data are woven into a tapestry of neural connections. These connections allow the model to grasp the subtleties of language, deciphering context, nuance, and even the whispers of sarcasm.\n",
      "\n",
      "The LLaMA 3's prowess lies in its ability to generate coherent, engaging prose on a wide range of topics, from the mundane to the sublime. Its capacity for creative expression is rivalled only by the greatest bards of Middle-earth, who could weave tales of wonder and magic with ease.\n",
      "\n",
      "However, beware, traveler, for as with all mighty powers, there lies a shadowy side to LLaMA 3's abilities. The model's vast knowledge and linguistic prowess can also lead it down treacherous paths, where the lines between truth and fiction blur like the mist-shrouded mountains of Moria.\n",
      "\n",
      "Thus, we must approach this wondrous technology with caution, lest we fall prey to its whims and fancies. Heed my words, dear traveler, and use LLaMA 3 wisely, that you may reap the rewards of its power while avoiding the perils that lie within.\n",
      "\n",
      "Now, tell me, fair traveler, what is it that thou seekest to know or discuss regarding this mighty language model? Shall we embark on a journey of discovery, or shall we converse on some other topic of thy choosing?\n"
     ]
    }
   ],
   "source": [
    "response = call_with_extra_prompts_information(full_prompt, temperature=0.3)\n",
    "print(f\"Response with extra prompts information: {response.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "052a9fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response with extra prompts information: ```json\n",
      "{\n",
      "  \"Model\": {\n",
      "    \"Nome\": \"Llama 3\",\n",
      "    \"Desenvolvedor\": \"Meta AI\",\n",
      "    \"Data de Lançamento\": \"2022\",\n",
      "    \"Características\": [\n",
      "      \"Arquitetura baseada em transformer\",\n",
      "      \"Suporte a múltiplas linguagens, incluindo inglês, espanhol e francês\",\n",
      "      \"Capacidade de processar grandes volumes de texto\"\n",
      "    ],\n",
      "    \"Aplicações\": [\n",
      "      \"Resposta a perguntas\",\n",
      "      \"Geração de texto\",\n",
      "      \"Tradução automática\"\n",
      "    ]\n",
      "  },\n",
      "  \"Desempenho\": {\n",
      "    \"Pontuação em termos de precisão\": \"Maior que 90%\",\n",
      "    \"Pontuação em termos de fluidez\": \"Maior que 85%\"\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "O Llama 3 é um modelo de linguagem baseado em transformer desenvolvido pela Meta AI. Ele foi lançado em 2022 e apresenta várias características únicas, incluindo uma arquitetura baseada em transformer e suporte a múltiplas linguagens.\n",
      "\n",
      "Algumas das principais características do Llama 3 incluem:\n",
      "\n",
      "*   Arquitetura baseada em transformer: O modelo utiliza uma estrutura de camadas para processar texto, o que permite uma maior eficiência e precisão.\n",
      "*   Suporte a múltiplas linguagens: O Llama 3 pode entender e gerar texto em várias línguas, incluindo inglês, espanhol e francês.\n",
      "*   Capacidade de processar grandes volumes de texto: O modelo é projetado para lidar com grandes quantidades de texto, o que o torna útil para aplicações como respostas a perguntas e geração de texto.\n",
      "\n",
      "O Llama 3 também tem várias aplicações práticas, incluindo:\n",
      "\n",
      "*   Resposta a perguntas: O modelo pode responder a perguntas de forma precisa e fluente.\n",
      "*   Geração de texto: O Llama 3 pode gerar texto automático em diferentes estilos e formatos.\n",
      "*   Tradução automática: O modelo pode traduzir texto de uma língua para outra com alta precisão.\n",
      "\n",
      "Em termos de desempenho, o Llama 3 apresenta pontuações excelentes em termos de precisão e fluidez. Ele é capaz de alcançar taxas de precisão maiores que 90% e taxas de fluidez maiores que 85%.\n"
     ]
    }
   ],
   "source": [
    "# We can also ask for the output to be in a specific format, language, or style.\n",
    "full_prompt = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that likes to talk about technologies and is very precise. Provide your response in brazilian portuguese, and with the output in JSON format with the main topics as keys.\"},\n",
    "    {\"role\": \"user\", \"content\": question_2}\n",
    "]\n",
    "response = call_with_extra_prompts_information(full_prompt, temperature=0.1)\n",
    "print(f\"Response with extra prompts information: {response.content}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
